{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27da0754b548450f8cfc67154c6fd319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,BitsAndBytesConfig\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"]=\"127.0.0.1:7890\"\n",
    "# os.environ[\"https_proxy\"]=\"127.0.0.1:7890\"\n",
    "\n",
    "model_id=\"Qwen/Qwen1.5-7B-Chat\"\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config,device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "\n",
    "text=\"For quantized models, we advise you to use the GPTQ, AWQ, and GGUF correspondents.\"\n",
    "\n",
    "\n",
    "def format_chat_prompt(message,chat_history):\n",
    "    prompt=''\n",
    "    for turn in chat_history:\n",
    "        user_message,bot_message=turn\n",
    "        prompt=f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt=f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def inferenceChat(message,chat_history):\n",
    "    formatted_prompt=format_chat_prompt(message,chat_history)\n",
    "    inputs=tokenizer(formatted_prompt,return_tensors='pt').to(device)\n",
    "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
    "    bot_message=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    chat_history.append((message,bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "def inference2C(text,outlen=512):\n",
    "    task=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with a sentence in English, and your task is to translate it into Chinese.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    chat_template=tokenizer.apply_chat_template(\n",
    "        task,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs=tokenizer([chat_template],return_tensors='pt').to(device)\n",
    "\n",
    "    generated_ids=model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=outlen\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response=tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def inference2E(text,outlen=512):\n",
    "    task=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with a sentence in Chinese, and your task is to translate it into English.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    chat_template=tokenizer.apply_chat_template(\n",
    "        task,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs=tokenizer([chat_template],return_tensors='pt').to(device)\n",
    "\n",
    "    generated_ids=model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=outlen\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response=tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def inferenceRw(text,outlen=512):\n",
    "    task=[\n",
    "        {\"role\": \"system\", \"content\": \"Rewrite the text without changing the meaning of the sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    chat_template=tokenizer.apply_chat_template(\n",
    "        task,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs=tokenizer([chat_template],return_tensors='pt').to(device)\n",
    "\n",
    "    generated_ids=model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=outlen\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response=tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def inferenceGramma(text,outlen=1024):\n",
    "    task=[\n",
    "        {\"role\": \"system\", \"content\": \"Correct grammatical errors of the text, do not modify them if there are no gramatical mistakes.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    chat_template=tokenizer.apply_chat_template(\n",
    "        task,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs=tokenizer([chat_template],return_tensors='pt').to(device)\n",
    "\n",
    "    generated_ids=model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=outlen\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response=tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def inferenceSummary(text,withinchar=100,outlen=512):\n",
    "    task=[\n",
    "        {\"role\": \"system\", \"content\": \"Make a summary of the text within {char} words\".format(char=withinchar)},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    chat_template=tokenizer.apply_chat_template(\n",
    "        task,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs=tokenizer([chat_template],return_tensors='pt').to(device)\n",
    "\n",
    "    generated_ids=model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=outlen\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response=tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a9235217295483156b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a9235217295483156b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 786, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 47.94 MiB is free. Including non-PyTorch memory, this process has 7.22 GiB memory in use. Of the allocated memory 6.67 GiB is allocated by PyTorch, and 353.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 680, in forward\n",
      "    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/cache_utils.py\", line 131, in update\n",
      "    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 46.25 MiB is free. Including non-PyTorch memory, this process has 7.24 GiB memory in use. Of the allocated memory 6.77 GiB is allocated by PyTorch, and 272.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 34.94 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 265.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 34.94 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.82 GiB is allocated by PyTorch, and 259.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 57.81 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.82 GiB is allocated by PyTorch, and 258.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 54.38 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 265.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 47.69 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 265.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 54.12 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 264.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 48.81 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.80 GiB is allocated by PyTorch, and 271.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 663, in forward\n",
      "    query_states = self.q_proj(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 51.44 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.80 GiB is allocated by PyTorch, and 271.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 786, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 7.32 GiB memory in use. Of the allocated memory 6.86 GiB is allocated by PyTorch, and 271.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 127, in inferenceSummary\n",
      "    generated_ids=model.generate(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 786, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 33.88 MiB is free. Including non-PyTorch memory, this process has 7.32 GiB memory in use. Of the allocated memory 6.73 GiB is allocated by PyTorch, and 403.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 786, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 34.19 MiB is free. Including non-PyTorch memory, this process has 7.32 GiB memory in use. Of the allocated memory 6.73 GiB is allocated by PyTorch, and 401.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 786, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 87.56 MiB is free. Including non-PyTorch memory, this process has 7.29 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 352.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 786, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 429, in forward\n",
      "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 70.19 MiB is free. Including non-PyTorch memory, this process has 7.29 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 352.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10719/1588685116.py\", line 17, in inferenceChat\n",
      "    outputs=model.generate(**inputs,max_new_tokens=512)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1173, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1058, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 773, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 680, in forward\n",
      "    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tony/anaconda3/lib/python3.11/site-packages/transformers/cache_utils.py\", line 132, in update\n",
      "    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 61.25 MiB is free. Including non-PyTorch memory, this process has 7.27 GiB memory in use. Of the allocated memory 6.79 GiB is allocated by PyTorch, and 280.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Assistants based on Qwen1.5-7B\")\n",
    "    gr.Markdown('## Translate English to Chinese')\n",
    "    gr.Interface(fn=inference2C,\n",
    "                  inputs=[gr.Textbox(label='English to Chinese.')],\n",
    "                  outputs=[gr.Textbox(label='Translation results.',show_label=True,show_copy_button=True)]\n",
    "                  )\n",
    "    gr.Markdown('## Translate Chinese to English')\n",
    "    gr.Interface(fn=inference2E,\n",
    "                  inputs=[gr.Textbox(label='Chinese to English.')],\n",
    "                  outputs=[gr.Textbox(label='Translation results.',show_label=True,show_copy_button=True)]\n",
    "                  )\n",
    "    gr.Markdown('## Rewrite')\n",
    "    gr.Interface(fn=inferenceRw,\n",
    "                  inputs=[gr.Textbox(label='Input text')],\n",
    "                  outputs=[gr.Textbox(label='Output',show_label=True,show_copy_button=True)]\n",
    "                  )\n",
    "    gr.Markdown('## Grammar correction')\n",
    "    gr.Interface(fn=inferenceGramma,\n",
    "                  inputs=[gr.Textbox(label='Input Text')],\n",
    "                  outputs=[gr.Textbox(label='Output',show_label=True,show_copy_button=True)]\n",
    "                  )\n",
    "    gr.Markdown('## Summary')\n",
    "    gr.Interface(fn=inferenceSummary,\n",
    "                  inputs=[gr.Textbox(label='Input Text'),\n",
    "                          gr.Slider(label='Max words',value=50,maximum=200,minimum=5)],\n",
    "                  outputs=[gr.Textbox(label='Output',show_label=True,show_copy_button=True)]\n",
    "                  )\n",
    "    gr.Markdown('## Just Chat')\n",
    "    chatbot=gr.Chatbot(height=240)\n",
    "    msg=gr.Textbox(label='Inputs')\n",
    "    btn=gr.Button('Submit')\n",
    "    clear=gr.ClearButton(components=[msg,chatbot],value='Clear')\n",
    "    btn.click(inferenceChat,inputs=[msg,chatbot],outputs=[msg,chatbot])\n",
    "    msg.submit(inferenceChat,inputs=[msg,chatbot],outputs=[msg,chatbot])\n",
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
